{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from err_gen import error_injection\n",
    "from utils import evaluate_opt\n",
    "from quant import quant, dequant\n",
    "from copy import deepcopy\n",
    "\n",
    "model_id = 'facebook/opt-125m'\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "cp_model = deepcopy(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "testenc = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original model accuracy(ppl): 27.579069137573242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'original model accuracy(ppl): {evaluate_opt(model.to(device), testenc)}')\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from err_gen import error_injection\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "bits = 8\n",
    "gs = 16\n",
    "scale, zero, qs = quant(bits, gs, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.001\n",
      "errored qweight model accuracy(ppl): 28.728588104248047\n",
      "Error rate: 0.002\n",
      "errored qweight model accuracy(ppl): 31.677818298339844\n",
      "Error rate: 0.003\n",
      "errored qweight model accuracy(ppl): 33.16647720336914\n",
      "Error rate: 0.004\n",
      "errored qweight model accuracy(ppl): 82.16188049316406\n",
      "Error rate: 0.005\n",
      "errored qweight model accuracy(ppl): 33.98486328125\n",
      "Error rate: 0.006\n",
      "errored qweight model accuracy(ppl): 42.31205749511719\n",
      "Error rate: 0.007\n",
      "errored qweight model accuracy(ppl): 50.46514892578125\n",
      "Error rate: 0.008\n",
      "errored qweight model accuracy(ppl): 44.27627182006836\n",
      "Error rate: 0.009000000000000001\n",
      "errored qweight model accuracy(ppl): 70.84103393554688\n",
      "Error rate: 0.01\n",
      "errored qweight model accuracy(ppl): 63.760005950927734\n"
     ]
    }
   ],
   "source": [
    "err_rate = np.linspace(1, 10, 10) * 1e-3\n",
    "loop_cnt = 1\n",
    "cnt = 0\n",
    "target_layer = [\n",
    "    'q_proj',\n",
    "    'k_proj',\n",
    "    'v_proj',\n",
    "    'out_proj',\n",
    "    'fc1',\n",
    "    'fc2',\n",
    "]\n",
    "\n",
    "for rate in err_rate:\n",
    "    print(f'Error rate: {rate}')\n",
    "    for loop in range(loop_cnt):\n",
    "        #print(f'Loop count: {loop+1}')\n",
    "        cp_qs = deepcopy(qs)\n",
    "        for key in cp_qs:\n",
    "            if key.split('.')[-1] in target_layer:\n",
    "                temp = error_injection(param=cp_qs[key].T[len(cp_qs[key].T)//2:], rate=rate, seed=int(42+cnt), device='cpu')\n",
    "                cp_qs[key].T[len(cp_qs[key].T)//2:] = temp\n",
    "                cnt+=1\n",
    "            \n",
    "        q_x_err = dequant(scale, zero, cp_qs, gs, bits)\n",
    "        cp_model = deepcopy(model)\n",
    "        for key in q_x_err.keys():\n",
    "            if key.split('.')[-1] != 'lm_head':\n",
    "                weight = key+'.weight'\n",
    "                cp_model.state_dict()[weight][:] = q_x_err[key]\n",
    "        \n",
    "        print(f'errored qweight model accuracy(ppl): {evaluate_opt(cp_model.to(device), testenc)}')\n",
    "        del q_x_err, cp_model\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
