{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import build_model_and_tokenizer\n",
    "from quant import quant, dequant\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "model_name = 'facebook/opt-6.7b'\n",
    "device = \"cuda:1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "testenc = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation 수집 후, q, k_proj을 통해 attn_map 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = next(model.parameters()).device # next는 객체의 __next__ 호출, 다음 iter를 부름?\n",
    "activation = {}\n",
    "\n",
    "def stat_tensor(name, tensor):\n",
    "    act_shape = tensor.shape\n",
    "    hidden_dim = tensor.shape[-1]\n",
    "    act_collect = tensor.detach().float().cpu()\n",
    "    #tensor = tensor.view(-1, hidden_dim).abs().detach()\n",
    "    if name in activation:\n",
    "        activation[name].append(act_collect)\n",
    "    else:\n",
    "        activation[name] = list()\n",
    "        activation[name].append(act_collect)\n",
    "\n",
    "\n",
    "def stat_input_hook(m, x, y, name):\n",
    "    if isinstance(x, tuple):\n",
    "        x = x[0]\n",
    "    stat_tensor(name, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: model.decoder.layers.X.self_attn.k_proj\n",
    "\n",
    "name: model.decoder.layers.X.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m, nn.Linear) | isinstance(m, Conv1D):\n",
    "        if name.split('.')[-1] == 'q_proj':\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    functools.partial(stat_input_hook, name=name))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset_list = []\n",
    "for ii in range(len(dataset)):\n",
    "    if dataset[ii]['text'] != '':\n",
    "        dataset_list.append(dataset[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:19<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 128\n",
    "seq_len = 512\n",
    "\n",
    "for i in tqdm(range(num_samples)):\n",
    "    input_ids = tokenizer(dataset_list[i][\"text\"], return_tensors=\"pt\",\n",
    "                            max_length=seq_len, truncation=True).input_ids.to(device)\n",
    "    model(input_ids)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
