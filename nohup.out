CUDA extension not installed.
CUDA extension not installed.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [10:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/jwjeong/workspace/simulation_robust_weight_map/scaled_act_analysis.py", line 90, in <module>
    model(input_ids)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwjeong/git/transformers/src/transformers/models/opt/modeling_opt.py", line 1051, in forward
    logits = self.lm_head(outputs[0]).contiguous()
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1595, in _call_impl
    hook_result = hook(self, args, result)
  File "/home/jwjeong/workspace/simulation_robust_weight_map/scaled_act_analysis.py", line 62, in stat_input_hook
    stat_tensor(name, x)
  File "/home/jwjeong/workspace/simulation_robust_weight_map/scaled_act_analysis.py", line 42, in stat_tensor
    for i in range(len(scale[name])):
KeyError: 'lm_head'
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 12.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.22s/it]
Traceback (most recent call last):
  File "scaled_act_analysis.py", line 17, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/home/jwjeong/workspace/git/transformers/src/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 11.73 GiB of which 41.38 MiB is free. Including non-PyTorch memory, this process has 11.68 GiB memory in use. Of the allocated memory 11.49 GiB is allocated by PyTorch, and 3.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
