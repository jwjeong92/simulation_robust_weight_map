{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import build_model_and_tokenizer\n",
    "from quant import quant, dequant\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "from transformers.pytorch_utils import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'facebook/opt-6.7b'\n",
    "device = \"cuda:1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "testenc = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.860100746154785"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 약 5분 30초 가량 소모\n",
    "'''\n",
    "from utils import opt_eval\n",
    "opt_eval(model, testenc, device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [01:16<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original ppl: 10.673616409301758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [01:16<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model ppl: 10.967483520507812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 약 1분 16초 소모, base model (fp) perplexity\n",
    "from utils import evaluate_opt, opt_eval #evaluate_opt: approx, opt_eval: more precise ppl\n",
    "print(f'original ppl: {evaluate_opt(model, testenc).item()}')\n",
    "\n",
    "# 약 28초 소모, cpu memory 약 67GB 까지 치솟음\n",
    "from quant import quant, dequant\n",
    "bits = 4\n",
    "gs = 128\n",
    "scale, zero, qs = quant(bits, gs, model)\n",
    "q_x = dequant(scale, zero, qs, gs, bits)\n",
    "for key in q_x.keys():\n",
    "    if key.split('.')[-1] != 'lm_head':\n",
    "        weight = key+'.weight'\n",
    "        model.state_dict()[weight][:] = q_x[key]\n",
    "\n",
    "# 1분 16초, quant_model perplexity\n",
    "print(f'quantized model ppl: {evaluate_opt(model, testenc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(model, dataset, tokenizer, num_samples=512, seq_len=512):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device # next는 객체의 __next__ 호출, 다음 iter를 부름?\n",
    "    act_scales = {}\n",
    "\n",
    "    def stat_tensor(name, tensor):\n",
    "        hidden_dim = tensor.shape[-1]\n",
    "        tensor = tensor.view(-1, hidden_dim).abs().detach()\n",
    "        comming_max = torch.max(tensor, dim=0)[0].float().cpu()\n",
    "        if name in act_scales:\n",
    "            act_scales[name] = torch.max(act_scales[name], comming_max)\n",
    "        else:\n",
    "            act_scales[name] = comming_max\n",
    "    \n",
    "    def stat_input_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        stat_tensor(name, x)\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear) | isinstance(m, Conv1D):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    functools.partial(stat_input_hook, name=name))\n",
    "            )\n",
    "    \n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    dataset_list = []\n",
    "    for ii in range(len(dataset)):\n",
    "        if dataset[ii]['text'] != '':\n",
    "            dataset_list.append(dataset[ii])\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        input_ids = tokenizer(dataset_list[i][\"text\"], return_tensors=\"pt\",\n",
    "                              max_length=seq_len, truncation=True).input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return act_ffn_min, act_ffn_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from err_gen import error_injection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
