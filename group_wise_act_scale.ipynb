{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwjeong/anaconda3/envs/autogptq/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 3.1s\n",
    "from utils import build_model_and_tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from quant import quant, dequant, quant_unpack, dequant_unpack\n",
    "import torch.nn as nn\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "import functools\n",
    "from functools import partial\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/opt-125m'\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "testenc = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = 8\n",
    "gs = 16\n",
    "scale, zero, qs = quant(bits, gs, model)\n",
    "q_x = dequant(scale, zero, qs, gs, bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([768, 48])\n",
      "torch.Size([3072, 48])\n",
      "torch.Size([768, 192])\n"
     ]
    }
   ],
   "source": [
    "for key in scale:\n",
    "    print(scale[key].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = next(model.parameters()).device # next는 객체의 __next__ 호출, 다음 iter를 부름?\n",
    "act_scales = {}\n",
    "scaled_act_max = {}\n",
    "scaled_act_min = {}\n",
    "scaled_act_sum = {}\n",
    "scaled_act_absum = {}\n",
    "scaled_act_numel = {}\n",
    "\n",
    "def stat_tensor(name, tensor):\n",
    "    hidden_dim = tensor.shape[-1]\n",
    "    tensor = tensor.view(-1, hidden_dim).detach()\n",
    "    act_shape = tensor.shape\n",
    "    for i in range(len(scale[name])):\n",
    "        temp = scale[name][i].expand(gs, -1).T.flatten().expand(act_shape[0], -1).to(device)\n",
    "        scaled_act = (tensor * temp)\n",
    "        if name in scaled_act_max:\n",
    "            scaled_act_max[name] = scaled_act.max() if scaled_act_max[name] < scaled_act.max() else scaled_act_max[name]\n",
    "            scaled_act_min[name] = scaled_act.min() if scaled_act_min[name] > scaled_act.min() else scaled_act_min[name]\n",
    "            scaled_act_sum[name] += scaled_act.sum()\n",
    "            scaled_act_absum[name] += torch.abs(scaled_act).sum()\n",
    "            scaled_act_numel[name] += torch.numel(scaled_act)\n",
    "        else:\n",
    "            scaled_act_max[name] = scaled_act.max()\n",
    "            scaled_act_min[name] = scaled_act.min()\n",
    "            scaled_act_sum[name] = scaled_act.sum()\n",
    "            scaled_act_absum[name] = torch.abs(scaled_act).sum()\n",
    "            scaled_act_numel[name] = torch.numel(scaled_act)\n",
    "\n",
    "\n",
    "def stat_input_hook(m, x, y, name):\n",
    "    if isinstance(x, tuple):\n",
    "        x = x[0]\n",
    "    stat_tensor(name, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m, nn.Linear) | isinstance(m, Conv1D):\n",
    "        if name.split('.')[-1] != 'lm_head':\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    functools.partial(stat_input_hook, name=name))\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset_list = []\n",
    "for ii in range(len(dataset)):\n",
    "    if dataset[ii]['text'] != '':\n",
    "        dataset_list.append(dataset[ii])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:24<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 4\n",
    "seq_len = 64\n",
    "\n",
    "for i in tqdm(range(num_samples)):\n",
    "    input_ids = tokenizer(dataset_list[i][\"text\"], return_tensors=\"pt\",\n",
    "                            max_length=seq_len, truncation=True).input_ids.to(device)\n",
    "    model(input_ids)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_act_avg = {}\n",
    "scaled_act_absavg = {}\n",
    "scaled_act_minmax = {}\n",
    "for key in scaled_act_max:\n",
    "    scaled_act_avg[key] = scaled_act_sum[key] / scaled_act_numel[key]\n",
    "    scaled_act_absavg[key] = scaled_act_absum[key] / scaled_act_numel[key]\n",
    "    scaled_act_minmax[key] = scaled_act_max[key] - scaled_act_min[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scaled_act_max, 'scaled_act/8bit/gs16/scaled_act_max_opt_125m.pt')\n",
    "torch.save(scaled_act_min, 'scaled_act/8bit/gs16/scaled_act_min_opt_125m.pt')\n",
    "torch.save(scaled_act_minmax, 'scaled_act/8bit/gs16/scaled_act_minmax_opt_125m.pt')\n",
    "torch.save(scaled_act_avg, 'scaled_act/8bit/gs16/scaled_act_avg_opt_125m.pt')\n",
    "torch.save(scaled_act_absavg, 'scaled_act/8bit/gs16/scaled_act_absavg_opt_125m.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
